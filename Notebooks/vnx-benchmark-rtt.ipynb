{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Round Trip time experiment (point to point)\n",
    "This notebook will show you how to measure the round trip time between two Alveo nodes using the benchmark application with UDP as a transport protocol.\n",
    "We are going to rely on a Dask cluster to configure the local and remote Alveo cards.\n",
    "\n",
    "This notebook assumes:\n",
    "* Direct connection between the Alveo cards\n",
    "* Dask cluster is already created and running. For more information about setting up a Dask cluster visit the [Dask documentation](https://docs.dask.org/en/latest/setup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.212.126:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.212.126:8787/status' target='_blank'>http://10.1.212.126:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>32</li>\n",
       "  <li><b>Memory: </b>232.35 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.212.126:8786' processes=2 threads=32, memory=232.35 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://10.1.212.126:8786\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_info = client.scheduler_info()['workers']\n",
    "workers = []\n",
    "for cli in client_info:\n",
    "    workers.append(client_info[cli]['name'])\n",
    "\n",
    "if len(workers) != 2:\n",
    "    print(\"Configure your Dask cluster with two workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic remote functions\n",
    "In this part we are going to schedule a basic function to the workers to verify that we are able to pinpoint tasks to a particular worker, we are also going to grab the Alveo shell name.\n",
    "You should visually check that your xclbin file is built for the Alveo shell available on the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker name: alveo3b | shell version: \"xilinx_u280_xdma_201920_3\"\n",
      "Worker name: alveo3c | shell version: \"xilinx_u280_xdma_201920_3\"\n"
     ]
    }
   ],
   "source": [
    "import platform, os\n",
    "\n",
    "def verify_workers():\n",
    "    node_name = platform.node()\n",
    "    shell_version = os.popen(\"xbutil dump | grep dsa_name\").read()\n",
    "    #match = True\n",
    "    #if 'xilinx_u280_xdma_201920_3' not in shell_version:\n",
    "    #    match = False\n",
    "    return node_name, shell_version[24:-2]\n",
    "\n",
    "worker_0 = client.submit(verify_workers ,workers=workers[0], pure=False)\n",
    "worker_1 = client.submit(verify_workers ,workers=workers[1], pure=False)\n",
    "\n",
    "worker_check = [worker_0.result(),worker_1.result()]\n",
    "\n",
    "for w in worker_check:\n",
    "    print('Worker name: {} | shell version: {}'.format(w[0],w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Dask device and utilities\n",
    "\n",
    "In this section we will declare the Dask code that builds on top of the `pynq` framework. This piece of code allow us to:\n",
    "\n",
    "* Download a `xclbin` file to a worker\n",
    "* Peek and poke registers\n",
    "* Allocate buffers\n",
    "* Start kernels\n",
    "\n",
    "All of these capabilities are available for both local and remote workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vnx_utils import *\n",
    "import pynq\n",
    "import ctypes\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from pynq import *\n",
    "from pynq.pl_server.xrt_device import XrtStream\n",
    "\n",
    "# Hold references to buffers to avoid them being collected\n",
    "# Won't be visible in the process but is an easy way to\n",
    "# let workers hold on to local references\n",
    "buffers = []\n",
    "\n",
    "# Functions that will be called in the context of dask\n",
    "def _invalidate(bo, offset, size):\n",
    "    buf = bytearray(size)\n",
    "    pynq.Device.active_device.invalidate(bo, offset, 0, size)\n",
    "    pynq.Device.active_device.buffer_read(bo, offset, buf)\n",
    "    return bytes(buf)\n",
    "\n",
    "def _flush(bo, offset, size, data):\n",
    "    pynq.Device.active_device.buffer_write(bo, offset, bytearray(data))\n",
    "    pynq.Device.active_device.flush(bo, offset, 0, size)\n",
    "    \n",
    "def _read_registers(address, length):\n",
    "    return pynq.Device.active_device.read_registers(address, length)\n",
    "\n",
    "def _write_registers(address, data):\n",
    "    pynq.Device.active_device.write_registers(address, data)\n",
    "    \n",
    "def _download(bitstream_data):\n",
    "    with tempfile.NamedTemporaryFile() as f:\n",
    "        f.write(bitstream_data)\n",
    "        f.flush()\n",
    "        ol = pynq.Overlay(f.name)\n",
    "\n",
    "def _alloc(size, memdesc):\n",
    "    mem = pynq.Device.active_device.get_memory(memdesc)\n",
    "    buf = mem.allocate((size,), 'u1')\n",
    "    buffers.append(buf)\n",
    "    return buf.bo, buf.device_address\n",
    "\n",
    "class DaskMemory:\n",
    "    \"\"\"Memory object proxied over dask\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, device, desc):\n",
    "        self._desc = desc\n",
    "        self._device = device\n",
    "    \n",
    "    def allocate(self, shape, dtype):\n",
    "        print(\"DaskMemory allocate\")\n",
    "        #print(\"Allocate method: shape {}\\tdtype {}\\tdevice {}\".format(shape,dtype, self._device))\n",
    "        from pynq.buffer import PynqBuffer\n",
    "        buf = PynqBuffer(shape, dtype, device_address=0,\n",
    "                         bo=0, device=self._device, coherent=False)\n",
    "        print(\"self._device {}\".format(self._device))\n",
    "        bo, addr = self._device._call_dask(_alloc, buf.nbytes, self._desc)\n",
    "        buf.bo = bo\n",
    "        buf.device_address = addr\n",
    "        return buf\n",
    "\n",
    "class DaskDevice(pynq.Device):\n",
    "    \"\"\"PYNQ Proxy device for using PYNQ via dask\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, client, worker):\n",
    "        \"\"\"The worker ID should be unique\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__(\"dask-\" + worker)\n",
    "        self._dask_client = client\n",
    "        self._worker = worker\n",
    "        self.capabilities = {\n",
    "            'REGISTER_RW': True,\n",
    "            'CALLABLE': True\n",
    "        }\n",
    "        self._streams = {}\n",
    "        \n",
    "    def _call_dask(self, func, *args):\n",
    "        future = self._dask_client.submit(func, *args, workers=self._worker, pure=False)\n",
    "        return future.result()\n",
    "\n",
    "    def invalidate(self, bo, offset, ptr, size):\n",
    "        ctype = ctypes.c_uint8 * size\n",
    "        target = ctype.from_address(ptr)\n",
    "        target[:] = self._call_dask(_invalidate, bo, offset, size)\n",
    "        \n",
    "    def flush(self, bo, offset, ptr, size):\n",
    "        ctype = ctypes.c_uint8 * size\n",
    "        target = ctype.from_address(ptr)\n",
    "        self._call_dask(_flush, bo, offset, size, bytes(target))\n",
    "        \n",
    "    def read_registers(self, address, length):\n",
    "        return self._call_dask(_read_registers, address, length)\n",
    "    \n",
    "    def write_registers(self, address, data):\n",
    "        self._call_dask(_write_registers, address, bytes(data))\n",
    "        \n",
    "    def get_bitfile_metadata(self, bitfile_name):\n",
    "        return pynq.pl_server.xclbin_parser.XclBin(bitfile_name)\n",
    "    \n",
    "    def download(self, bitstream, parser=None):\n",
    "        with open(bitstream.bitfile_name, 'rb') as f:\n",
    "            bitstream_data = f.read()\n",
    "        self._call_dask(_download, bitstream_data)\n",
    "        super().post_download(bitstream, parser)\n",
    "    \n",
    "    def get_memory(self, desc):\n",
    "        if desc['streaming']:\n",
    "            if desc['idx'] not in self._streams:\n",
    "                self._streams[desc['idx']] = XrtStream(self, desc)\n",
    "            return self._streams[desc['idx']]\n",
    "        else:\n",
    "            return DaskMemory(self, desc)\n",
    "    \n",
    "    def get_memory_by_idx(self, idx):\n",
    "        #print(\"get_memory_by_idx {}\".format(idx))\n",
    "        for m in self.mem_dict.values():\n",
    "            if m['idx'] == idx:\n",
    "                return self.get_memory(m)\n",
    "        raise RuntimeError(\"Could not find memory\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download xclbin to workers\n",
    "1. Create Dask device for each worker\n",
    "2. Create an overlay object for each worker, this step will download the `xclbin` file to the Alveo card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/scratch/marruiz/conda/lib/python3.7/site-packages/distributed/worker.py:3321: UserWarning: Large object of size 50.18 MB detected in task graph: \n",
      "  (b'xclbin2\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff ... ROR_DATA_END',)\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n"
     ]
    }
   ],
   "source": [
    "daskdev_w0 = DaskDevice(client, workers[0])\n",
    "daskdev_w1 = DaskDevice(client, workers[1])\n",
    "\n",
    "xclbin = '../xup_network/xup_vitis_networking_if11111.xclbin'\n",
    "ol_w0 = pynq.Overlay(xclbin, device=daskdev_w0)\n",
    "ol_w1 = pynq.Overlay(xclbin, device=daskdev_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Link \n",
    "\n",
    "We are going to use the function `linkStatus` that reports if the CMAC is detecting link, which means that the physical connection\n",
    "between the two Alveo cards is established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link worker 0 {'cmac_link': True}; link worker 1 {'cmac_link': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"Link worker 0 {}; link worker 1 {}\".format(linkStatus(ol_w0.cmac_1),linkStatus(ol_w1.cmac_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure remote worker\n",
    "\n",
    "1. Set up IP address and MAC address\n",
    "2. Set up connection table\n",
    "3. Launch ARP discovery\n",
    "4. Print out ARP Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP address 192.168.0.10, MAC 0xa35029dea\n",
      "Position   5\tMAC address 00:0a:35:02:9d:e5\tIP address 192.168.0.5\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "updateIPAddress(ol_w1.networklayer_1, '192.168.0.10')\n",
    "print (\"IP address {}, MAC {}\".format(ipaddress.IPv4Address(int(ol_w1.networklayer_1.register_map.ip_address)),hex(ol_w1.networklayer_1.register_map.mac_address)))\n",
    "# 2\n",
    "socketType = np.dtype([('theirIP', np.uint32), ('theirPort', np.uint16), ('myPort', np.uint16), ('valid', np.bool)])\n",
    "socketw1 = np.zeros(16, dtype=socketType)\n",
    "socketw1[6] = (0xC0A80005, 62177, 60512, True)\n",
    "initSocketTable(ol_w1, socketw1, interface = 1, device = daskdev_w1, debug=False)\n",
    "#3 \n",
    "\n",
    "ol_w1.networklayer_1.register_map.arp_discovery = 1\n",
    "ol_w1.networklayer_1.register_map.arp_discovery = 0\n",
    "#4\n",
    "readARPTable(ol_w1, 1, 256, device=daskdev_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure local worker\n",
    "\n",
    "1. Print out IP and MAC address\n",
    "2. Set up connection table\n",
    "3. Launch ARP discovery\n",
    "4. Print out ARP Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP address 192.168.0.5, MAC 0xa35029de5\n",
      "Position  10\tMAC address 00:0a:35:02:9d:ea\tIP address 192.168.0.10\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "print (\"IP address {}, MAC {}\".format(ipaddress.IPv4Address(int(ol_w0.networklayer_1.register_map.ip_address)),hex(ol_w0.networklayer_1.register_map.mac_address)))\n",
    "#2\n",
    "socketw0 = np.zeros(16, dtype=socketType)\n",
    "socketw0[3] = (0xC0A8000a, 60512, 62177, True)\n",
    "initSocketTable(ol_w0, socketw0, interface = 1, device=daskdev_w0, debug=False)\n",
    "\n",
    "#3 \n",
    "ol_w0.networklayer_1.register_map.arp_discovery = 1\n",
    "ol_w0.networklayer_1.register_map.arp_discovery = 0\n",
    "#4\n",
    "readARPTable(ol_w0, 1, 256, device=daskdev_w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure application\n",
    "\n",
    "* Configure remote benchmark application in `LOOPBACK` mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_w1.traffic_generator_1.register_map.mode = benchmark_mode.index('LOOPBACK')\n",
    "ol_w1.traffic_generator_1.register_map.dest_id = 6 # Use connection in position 6 to reflect\n",
    "ol_w1.traffic_generator_1.register_map.CTRL.AP_START = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure local benchmark application\n",
    "This part configures the collector, in particular\n",
    "* Allocate buffers\n",
    "* Start collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaskMemory allocate\n",
      "self._device <__main__.DaskDevice object at 0x7f8ee82a6ad0>\n",
      "DaskMemory allocate\n",
      "self._device <__main__.DaskDevice object at 0x7f8ee82a6ad0>\n"
     ]
    }
   ],
   "source": [
    "send_packets   = 2 ** 20\n",
    "shape          = (send_packets,1)\n",
    "rtt_cycles     = pynq.allocate(shape, dtype=np.uint32, target=ol_w0.HBM0)\n",
    "pkt            = pynq.allocate(1,     dtype=np.uint32, target=ol_w0.HBM0)\n",
    "\n",
    "collector_h = ol_w0.collector_1.start(rtt_cycles,pkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**This part configures the traffic generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_pkts = send_packets\n",
    "ol_w0.traffic_generator_1.register_map.debug_reset = 1\n",
    "ol_w0.networklayer_1.register_map.debug_reset_counters = 1\n",
    "ol_w0.traffic_generator_1.register_map.mode = benchmark_mode.index('LATENCY')\n",
    "ol_w0.traffic_generator_1.register_map.number_packets = send_pkts\n",
    "ol_w0.traffic_generator_1.register_map.time_between_packets = 50\n",
    "ol_w0.traffic_generator_1.register_map.number_beats = 1\n",
    "ol_w0.traffic_generator_1.register_map.dest_id = 3\n",
    "ol_w0.traffic_generator_1.register_map.CTRL.AP_START = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read latency result\n",
    "* Call the dask method to synchronize the Alveo buffer with the dask buffer\n",
    "\n",
    "Note that this buffer contains the round trip time in clock cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PynqBuffer([[314],\n",
       "            [313],\n",
       "            [315],\n",
       "            ...,\n",
       "            [313],\n",
       "            [314],\n",
       "            [314]], dtype=uint32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtt_cycles.invalidate()\n",
    "rtt_cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute some statistics on the results\n",
    "1. Convert the rtt from cycles to microseconds, in this case the clock frequency is 300 MHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 300\n",
    "period=  1/(freq * 10**6)\n",
    "rtt_sec = np.array(shape, dtype=np.float)\n",
    "rtt_sec= rtt_cycles / freq  # make compute in microseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use scipy to compute statistical values\n",
    "    * Mean\n",
    "    * Standard deviation\n",
    "    * Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round trip time at application level using 1,048,576 packets\n",
      "\tmean    = 1.048 us\n",
      "\tstd_dev = 0.003943 us\n",
      "\tmode    = 1.050 us, which appears 338,171 times\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "mean, std_dev, mode = np.mean(rtt_sec), np.std(rtt_sec), stats.mode(rtt_sec)\n",
    "print(\"Round trip time at application level using {:,} packets\".format(len(rtt_sec)))\n",
    "print(\"\\tmean    = {:.3f} us\\n\\tstd_dev = {:.6f} us\".format(mean,std_dev))\n",
    "print(\"\\tmode    = {:.3f} us, which appears {:,} times\".format(mode[0][0][0],mode[1][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release Alveo cards\n",
    "* To release the alveo cards the pynq overlay is freed\n",
    "* Delete dask pynq-dask buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pynq.Overlay.free(ol_w0)\n",
    "pynq.Overlay.free(ol_w1)\n",
    "del rtt_cycles\n",
    "del pkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "Copyright (c) 2020, Xilinx, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
